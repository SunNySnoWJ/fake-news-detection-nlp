{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP8XvY2hV0XX3ZS89wP525o"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L6EmP9pg6-zu","executionInfo":{"status":"ok","timestamp":1753083562065,"user_tz":240,"elapsed":21039,"user":{"displayName":"Meet Adalja","userId":"05187839259704476211"}},"outputId":"f94069a5-c64a-4b39-f8f9-da2204e73ce0"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":6,"metadata":{"id":"g_FpR4R75tpx","executionInfo":{"status":"ok","timestamp":1753083566859,"user_tz":240,"elapsed":10,"user":{"displayName":"Meet Adalja","userId":"05187839259704476211"}}},"outputs":[],"source":["import pandas as pd\n","\n","def load_fakenewsnet_data():\n","    # Load all CSVs\n","    gc_fake = pd.read_csv('/content/drive/MyDrive/FakeNewsNet/dataset/gossipcop_fake.csv')\n","    gc_real = pd.read_csv('/content/drive/MyDrive/FakeNewsNet/dataset/gossipcop_real.csv')\n","    pf_fake = pd.read_csv('/content/drive/MyDrive/FakeNewsNet/dataset/politifact_fake.csv')\n","    pf_real = pd.read_csv('/content/drive/MyDrive/FakeNewsNet/dataset/politifact_real.csv')\n","\n","    # Add domain and label columns\n","    gc_fake['domain'] = 'gossipcop'\n","    gc_real['domain'] = 'gossipcop'\n","    pf_fake['domain'] = 'politifact'\n","    pf_real['domain'] = 'politifact'\n","\n","    gc_fake['label'] = 0\n","    gc_real['label'] = 1\n","    pf_fake['label'] = 0\n","    pf_real['label'] = 1\n","\n","    # Combine into a single DataFrame\n","    df = pd.concat([gc_fake, gc_real, pf_fake, pf_real], ignore_index=True)\n","    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n","\n","    return df\n"]},{"cell_type":"code","source":["import pandas as pd\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from textblob import TextBlob\n","\n","# Download required NLTK resources\n","nltk.download('stopwords')\n","nltk.download('punkt_tab')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","\n","stop_words = set(stopwords.words('english'))\n","lemmatizer = WordNetLemmatizer()\n","\n","# Clean a single text string\n","def clean_text(text):\n","    if pd.isnull(text):\n","        return \"\"\n","    text = text.lower()\n","    text = re.sub(r\"http\\S+\", \"\", text)  # remove URLs\n","    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # remove non-alphabetic characters\n","    tokens = nltk.word_tokenize(text)\n","    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words and len(w) > 2]\n","    return \" \".join(tokens)\n","\n","# Add NLP features like sentiment and length\n","def add_nlp_features(df):\n","    df['clean_title'] = df['title'].apply(clean_text)\n","\n","    if 'text' in df.columns:\n","        df['clean_text'] = df['text'].apply(clean_text)\n","        df['text_len'] = df['clean_text'].apply(lambda x: len(x.split()))\n","        df['text_sentiment'] = df['clean_text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n","    else:\n","        df['text_len'] = df['clean_title'].apply(lambda x: len(x.split()))\n","        df['text_sentiment'] = df['clean_title'].apply(lambda x: TextBlob(x).sentiment.polarity)\n","\n","    # Optional: parse publish date\n","    if 'publish_date' in df.columns:\n","        df['publish_date'] = pd.to_datetime(df['publish_date'], errors='coerce')\n","        df['publish_year'] = df['publish_date'].dt.year\n","        df['publish_month'] = df['publish_date'].dt.month\n","\n","    return df\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h9wqTCEl6SdR","executionInfo":{"status":"ok","timestamp":1753083587398,"user_tz":240,"elapsed":290,"user":{"displayName":"Meet Adalja","userId":"05187839259704476211"}},"outputId":"d58898d7-8b70-4d88-d5f6-a2e57231a182"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]}]},{"cell_type":"code","source":["df = load_fakenewsnet_data()\n","df = add_nlp_features(df)\n","\n","print(\"Cleaned Columns:\", df.columns.tolist())\n","print(df[['label', 'clean_title', 'text_len', 'text_sentiment']].head())\n","\n","# Optional: save in Colab\n","df.to_csv('cleaned_fakenewsnet.csv', index=False)\n","print(\"✅ Cleaned dataset saved to: cleaned_fakenewsnet.csv\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"37K1LrUZ6jb5","executionInfo":{"status":"ok","timestamp":1753083605624,"user_tz":240,"elapsed":15706,"user":{"displayName":"Meet Adalja","userId":"05187839259704476211"}},"outputId":"5a7814bd-ad7a-4dc7-afec-c3f6e2b071b4"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Cleaned Columns: ['id', 'news_url', 'title', 'tweet_ids', 'domain', 'label', 'clean_title', 'text_len', 'text_sentiment']\n","   label                                        clean_title  text_len  \\\n","0      0  bindi irwin get married boyfriend chandler powell         7   \n","1      1       bob harper howard stern reached heart attack         7   \n","2      1  guardian galaxy vol cast play guess guardian j...        10   \n","3      1                       xfiles scully whisper mulder         4   \n","4      1  today rating show replacing matt lauer hoda ko...        10   \n","\n","   text_sentiment  \n","0        0.250000  \n","1        0.000000  \n","2        0.136364  \n","3        0.000000  \n","4        0.500000  \n","✅ Cleaned dataset saved to: cleaned_fakenewsnet.csv\n"]}]},{"cell_type":"code","source":["from google.colab import files\n","files.download('cleaned_fakenewsnet.csv')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"CCYja5N17bO2","executionInfo":{"status":"ok","timestamp":1753083656614,"user_tz":240,"elapsed":37,"user":{"displayName":"Meet Adalja","userId":"05187839259704476211"}},"outputId":"6b8cb64f-a85e-4d48-825e-87aabd3485b1"},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_c3e42d25-26ce-4436-8812-ec3b265e824c\", \"cleaned_fakenewsnet.csv\", 45821530)"]},"metadata":{}}]}]}